---
title: "p8105_hw6_jy2944"
author: "Jie Yu"
date: "11/25/2018"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(mgcv)


knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%",
  warning = FALSE,
  message = FALSE
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1

### Import and tidy data

Read data through a [GitHub repository](https://github.com/washingtonpost/data-homicides). Create a `city_state` variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. Modifiy `victim_race` to have categories `white` and `non-white`, with `white` as the reference category. Be sure that `victim_age` is numeric.

```{r p1_tidy}
# Read data through a Github repo
homicide_data = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  mutate(
    # Create a `city_state` variable (e.g. “Baltimore, MD”)
    city_state = str_c(city, ",", state),
    # Create a binary variable indicating whether the homicide is solved: 1 - solved; 0 - unsolved
    resolved = as.numeric(disposition == "Closed by arrest")
    ) %>% 
  # Omit some cities
  filter(!city_state %in% c("Dallas,TX","Phoenix,AZ", "Kansas City,MO", "Tulsa,AL")) %>%
  mutate(
    # Modifiy `victim_race` to have categories `white` and `non-white`, with `white` as the reference
    victim_race = fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white"),
    # Be sure that `victim_age` is numeric
    victim_age = as.numeric(victim_age)
    )

str(homicide_data)
```

### Fit a logistic regression for one city

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race (as just defined) as predictors. Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and CI of the **adjusted odds ratio** for solving homicides comparing non-white victims to white victims keeping all other variables fixed.

```{r p1_balti}
balti_logistic = homicide_data %>% 
  # filter the city
  filter(city_state == "Baltimore,MD") %>% 
  # Fit a logistic regression: `resolved` as outcome
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 

balti_logistic %>% 
  broom::tidy() %>% 
  # Obtain odds ratio and its CI
  # Note: logistic model estimates are log odds ratios, so we need to tranform them back
  mutate(
    OR = exp(estimate),
    conf.lower = exp(estimate - std.error * 1.96),
    conf.upper = exp(estimate + std.error * 1.96)
    ) %>%
  select(term, OR, conf.lower, conf.upper, p.value) %>% 
  knitr::kable(digits = 3)
```

The odds ratio for solving homicides comparing non-white victims to white victims is 0.441 and the 95% confidence interval is (0.312, 0.620). It means that homicides in which the victim was non-white were substantially less likely to be resolved than those in which the victim was white (OR = 0.441 < 1, p-value < 0.001). The non-white victims had 0.441 times the odds of having their cases being resolved compared to the white victims. The true odd ratio is between 0.312 and 0.620.

### Fit logistic regression for each of the cities

Now run `glm` for each of the cities in the dataset, and extract the adjusted odds ratio and CI for solving homicides comparing non-white victims to white victims. Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and `unnest` as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r p1_each_city}
# Contruct a function `or_and_ci` to calculate OR and CI using `glm` results
or_and_ci = function(df){
    glm = glm(resolved ~ victim_age + victim_sex + victim_race, data = df, family = binomial())
    
    glm %>% 
      broom::tidy() %>% 
      mutate(
        OR = exp(estimate),
        conf.lower = exp(estimate - std.error * 1.96),
        conf.upper = exp(estimate + std.error * 1.96)
        ) %>%
      select(term, OR, conf.lower, conf.upper, p.value) %>% 
      # filter the term of non-white victims
      filter(term == "victim_racenon-white") %>% 
      select(-term)
    }

city_logistic = homicide_data %>% 
  # nest the data by each city
  group_by(city_state) %>% 
  nest() %>% 
  # Apply the function `or_and_ci` to each city
  mutate(or_and_ci = purrr::map(.x = data, ~or_and_ci(.x))) %>% 
  select(-data) %>% 
  unnest()

city_logistic
```


### Plot showing ORs and CIs for each city

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r p1_plot, fig.width = 10, fig.height = 6}
city_logistic %>% 
  # # organize cities according to estimated OR
  mutate(city_state = forcats::fct_reorder(city_state, OR)) %>% 
  # make the plot
  ggplot(aes(x = city_state, y = OR)) + 
  # points represent the ORs for each city
  geom_point(color = "red") + 
  # geom_errorbar(): add error bars based on the upper and lower limits
  geom_errorbar(aes(x = city_state, ymin = conf.lower, ymax = conf.upper)) + 
  labs(
    title = "Estimated ORs and CIs for solving homicides comparing non-white victims to white victims", 
    x = "City", 
    y = "Estimate odds ratio") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Estimated odds ratio and its 95% confidence interval for solving homices comparing non-white victioms to white victims varies across cities in US. Most of the cities have estimated odds ratios lower than 1, meaning that homicides in which the victim is non-white are less likely to be resolved than those in which the victim is white in US. Boston, MA has the lowest estimated odds ratio while Tampa, FL has the highest estimated odds ratio.

# Problem 2

This problem focuses on the effects of several variables on a child's birthweight.

### Import and tidy data

```{r}
birth_weight = read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>% 
  # convert some categorical data from integer to factor
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )

# Check if there is missing data
skimr::skim(birth_weight )
```

After importing the data, I convert `babysex`, `frace`, `malform` and `mrace` from integer to factor because they are categorical data. There is no missing data in the dataset.

The explanations of each variable are as follows:

* `babysex`: baby’s sex (male = 1, female = 2)
* `bhead`: baby’s head circumference at birth (centimeters)
* `blength`: baby’s length at birth (centimeteres)
* `bwt`: baby’s birth weight (grams)
* `delwt`: mother’s weight at delivery (pounds)
* `fincome`: family monthly income (in hundreds, rounded)
* `frace`: father’s race (1= White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
* `gaweeks`: gestational age in weeks
* `malform`: presence of malformations that could affect weight (0 = absent, 1 = present)
* `menarche`: mother’s age at menarche (years)
* `mheigth`: mother’s height (inches)
* `momage`: mother’s age at delivery (years)
* `mrace`: mother’s race (1= White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
* `parity`: number of live births prior to this pregnancy
* `pnumlbw`: previous number of low birth weight babies
* `pnumgsa`: number of prior small for gestational age babies
* `ppbmi`: mother’s pre-pregnancy BMI
* `ppwt`: mother’s pre-pregnancy weight (pounds)
* `smoken`: average number of cigarettes smoked per day during pregnancy
* `wtgain`: mother’s weight gain during pregnancy (pounds)


### Propose a regression model for birthweight

I use backward stepwise regression to select the model. I choose backward search because the MSE tends to be unbiased when important predictors are retained at each step.

```{r p2_model_select}
fit_all <- lm(bwt ~ ., data = birth_weight)
step(fit_all, direction = "backward")
```

The "best" model selected by the stepwise backward stepwise regression is `bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken`. We then check colinearity among its predictors using variance inflation factors (VIF), and look at the summary of the model.

```{r p2_vif}
# "best" model selected
fit_best <- lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                 gaweeks + mheight + mrace + parity + ppwt + smoken, data = birth_weight)

# Identify collinearity among predictors
HH::vif(fit_best)
```

```{r p2_model_summary}
# summary the model
summary(fit_best)
```

All of the VIFs calculated for each predictor are lower than 5, indicating that the coefficients might not be misleading due to collinearity. The adjusted $R^{2}$ is 0.7173, which is good for model building, and most of the predictors have significant coefficient.


### Plot of model residuals against fitted values

Show a plot of model residuals against fitted values (predicted value) – use `add_predictions` and `add_residuals` in making this plot.

```{r}
birth_weight %>% 
  modelr::add_predictions(model = fit_best) %>% 
  modelr::add_residuals(model = fit_best) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Model residuals against fitted values",
        y = "Residual",
        x = "Prediction value"
    )
```

The above plot suggests that the residual become higher at lower predicted value, meaning that the model will not fit the data well and there will be some outliers when the predicted value goes to small.


### Compare models

Compare your model to two others:

* One using length at birth and gestational age as predictors (main effects only)

* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

Make this comparison in terms of the cross-validated prediction error; use `crossv_mc` and functions in `purrr` as appropriate.

```{r}
# generate 100 pairs training and testing sets
cv_df = crossv_mc(birth_weight, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
  
  
cv_df = cv_df %>% 
  mutate(
    # model_1: the model I selected
    model_1 = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                 gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    # model_2: main effects only
    model_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    # model_3: including interactions
    model_3 = map(train, ~lm(bwt ~ babysex + blength + bhead + babysex * blength + babysex * bhead + 
                                     blength * bhead + babysex * blength * bhead, data = .x))
        ) %>% 
  mutate(
    rmse_model_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_model_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_model_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y))
    )

cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin() +
  stat_summary(fun.y = mean, geom = "point", color = "blue", size = 3) + 
  labs(
    title = "The distribution of RMSE values for each model",
    x = "Model",
    y = "RMSE"
    )
  
```

The plot suggests that the model I selected (`model_1`) is a clear winner because it has lowest mean RMSE value and the overall distribution of RMSE values is relatively low. The model using main effects only (`model_2`) is the worst because its RMSE values are really high. The model including three-way intersections(`model_3`) is better than the main effect model because it has a rather lower distribution of RMSE values.

