---
title: "p8105_hw6_jy2944"
author: "Jie Yu"
date: "11/25/2018"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(mgcv)


knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%",
  warning = FALSE,
  message = FALSE
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1

### Import and tidy data

Read data through a [GitHub repository](https://github.com/washingtonpost/data-homicides). Create a `city_state` variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. Modifiy `victim_race` to have categories `white` and `non-white`, with `white` as the reference category. Be sure that `victim_age` is numeric.

```{r p1_tidy}
# Read data through a Github repo
homicide_data = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  mutate(
    # Create a `city_state` variable (e.g. “Baltimore, MD”)
    city_state = str_c(city, ",", state),
    # Create a binary variable indicating whether the homicide is solved: 1 - solved; 0 - unsolved
    resolved = as.numeric(disposition == "Closed by arrest")
    ) %>% 
  # Omit some cities
  filter(!city_state %in% c("Dallas,TX","Phoenix,AZ", "Kansas City,MO", "Tulsa,AL")) %>%
  mutate(
    # Modifiy `victim_race` to have categories `white` and `non-white`, with `white` as the reference
    victim_race = fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white"),
    # Be sure that `victim_age` is numeric
    victim_age = as.numeric(victim_age)
    )
```

### Fit a logistic regression for one city

For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race (as just defined) as predictors. Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and CI of the **adjusted odds ratio** for solving homicides comparing non-white victims to white victims keeping all other variables fixed.

```{r p1_balti}
balti_logistic = homicide_data %>% 
  # filter the city
  filter(city_state == "Baltimore,MD") %>% 
  # Fit a logistic regression: `resolved` as outcome
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 

balti_logistic %>% 
  broom::tidy() %>% 
  # Obtain odds ratio and its CI
  # Note: logistic model estimates are log odds ratios, so we need to tranform them back
  mutate(
    OR = exp(estimate),
    conf.lower = exp(estimate - std.error * 1.96),
    conf.upper = exp(estimate + std.error * 1.96)
    ) %>%
  select(term, OR, conf.lower, conf.upper, p.value) %>% 
  knitr::kable(digits = 3)
```

The odds ratio for solving solving homicides comparing non-white victims to white victims is 0.441 and the CI is (0.312, 0.620) with 95% confidence interval. It means that homicides in which the victim was non-white were substantially less likely to be resolved than those in which the victim was white (OR = 0.441 < 1, p-value < 0.001). The non-white victims had 0.441 times the odds of having their cases being resolved compared to the white victims. The true odd ratio is between 0.312 and 0.620.

### Fit logistic regression for each of the cities

Now run `glm` for each of the cities in the dataset, and extract the adjusted odds ratio and CI for solving homicides comparing non-white victims to white victims. Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and `unnest` as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r p1_each_city}
# Contruct a function `or_and_ci` to calculate OR and CI using `glm` results
or_and_ci = function(df){
    glm = glm(resolved ~ victim_age + victim_sex + victim_race, data = df, family = binomial())
    
    logistic_tidy = glm %>% 
      broom::tidy() %>% 
      mutate(
        OR = exp(estimate),
        conf.lower = exp(estimate - std.error * 1.96),
        conf.upper = exp(estimate + std.error * 1.96)
        ) %>%
      select(term, OR, conf.lower, conf.upper, p.value) %>% 
      # filter the term of non-white victims
      filter(term == "victim_racenon-white") %>% 
      select(-term)
    }

city_logistic = homicide_data %>% 
  # nest the data by each city
  group_by(city_state) %>% 
  nest() %>% 
  # Apply the function `or_and_ci` to each city
  mutate(or_and_ci = map(.x = data, ~or_and_ci(.x))) %>% 
  select(-data) %>% 
  unnest()

city_logistic
```


### Plot showing ORs and CIs for each city

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r p1_plot, fig.width = 10, fig.height = 6}
city_logistic %>% 
  # # organize cities according to estimated OR
  mutate(city_state = forcats::fct_reorder(city_state, OR)) %>% 
  # make the plot
  ggplot(aes(x = city_state, y = OR)) + 
  # points represent the ORs for each city
  geom_point(color = "red") + 
  # geom_errorbar(): add error bars based on the upper and lower limits
  geom_errorbar(aes(x = city_state, ymin = conf.lower, ymax = conf.upper)) + 
  labs(
    title = "Estimated ORs and CIs for solving homicides comparing non-white victims to white victims", 
    x = "City", 
    y = "Estimate odds ratio") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Estimated odds ratio and its confidence interval for solving homices comparing non-white victioms to white victims varies across cities in US. Most of the cities have estimated odds ratios lower than 1, meaning that homicides in which the victim is non-white are less likely to be resolved than those in which the victim is white in US. Boston, MA has the lowest estimated odds ratio while Tampa, FL has the highest estimated odds ratio.

# Problem 2

This problem focuses on the effects of several variables on a child's birthweight.

### Import and tidy data

```{r}
birth_weight = read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() %>% 
  mutate(
    # convert some categorical data from integer to factor
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace),
    # convert birthweight unit from grams to pounds
    bwt = as.numeric(bwt * 0.00220462)
  )

# Check if there is missing data
skimr::skim(birth_weight )
```

After importing the data, I convert `babysex`, `frace`, `malform` and `mrace` from integer to factor because they are categorical data. I also convert the unit of `bwt` from grams to pounds to make it consistent with other weitht variables in the dataset. There is no missing data in the dataset.

Now, the explanations of each variable are as follows:

* `babysex`: baby’s sex (male = 1, female = 2)
* `bhead`: baby’s head circumference at birth (centimeters)
* `blength`: baby’s length at birth (centimeteres)
* `bwt`: baby’s birth weight (pounds)
* `delwt`: mother’s weight at delivery (pounds)
* `fincome`: family monthly income (in hundreds, rounded)
* `frace`: father’s race (1= White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
* `gaweeks`: gestational age in weeks
* `malform`: presence of malformations that could affect weight (0 = absent, 1 = present)
* `menarche`: mother’s age at menarche (years)
* `mheigth`: mother’s height (inches)
* `momage`: mother’s age at delivery (years)
* `mrace`: mother’s race (1= White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
* `parity`: number of live births prior to this pregnancy
* `pnumlbw`: previous number of low birth weight babies
* `pnumgsa`: number of prior small for gestational age babies
* `ppbmi`: mother’s pre-pregnancy BMI
* `ppwt`: mother’s pre-pregnancy weight (pounds)
* `smoken`: average number of cigarettes smoked per day during pregnancy
* `wtgain`: mother’s weight gain during pregnancy (pounds)


### Model Selection

I use backward stepwise regression to select the model. I choose backward search because the MSE tends to be unbiased when important predictors are retained at each step.

```{r p2_model_select}
fit_all <- lm(bwt ~ ., data = birth_weight)
step(fit_all, direction = "backward")
```

The "best" model selected by the stepwise backward stepwise regression is `bwt` ~ `babysex` + `bhead` + `blength`+ `gaweeks` + `delwt` + `mheight` + `mrace` + `parity` + `ppwt` + `smoken` + `fincome`. We then check colinearity among its predictors using variance inflation factors (VIF), and look at the summary of the model.

```{r p2_vif}
# "best" model selected
fit_best <- lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
                 gaweeks + mheight + mrace + parity + ppwt + smoken, data = birth_weight)

# Identify collinearity among predictors
HH::vif(fit_best)
```

```{r p2_model_summary}
# summary the model
summary(fit_best)
```

All of the VIFs calculated for each predictor are lower than 5, indicating that the coefficients might not be misleading due to collinearity. The adjusted $R^{2}$ is 0.7173, which is good for model building, and most of the predictors have significant coefficient.






